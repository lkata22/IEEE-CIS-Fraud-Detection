{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14242,"databundleVersionId":568274,"sourceType":"competition"}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install mlflow dagshub","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import dagshub\ndagshub.init(repo_owner='lkata22',\n             repo_name='IEEE-CIS-Fraud-Detection',\n             mlflow=True)\n\nimport mlflow","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Imports\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport gc\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import KFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import roc_auc_score\nfrom xgboost import XGBClassifier\n\n\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Memory Reducer to reduce memory usage of DataFrame","metadata":{}},{"cell_type":"code","source":"class MemoryReducer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        X = reduce_mem_usage(X)\n        return X\n\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtype\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose:\n        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(\n            end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cleaner Class","metadata":{}},{"cell_type":"code","source":"class MissingValueHandler(BaseEstimator, TransformerMixin):\n    def __init__(self, num_strategy='median', cat_strategy='constant', fill_value='missing'):\n        self.num_strategy = num_strategy\n        self.cat_strategy = cat_strategy\n        self.fill_value = fill_value\n        self.num_impute_values = {}\n        self.cat_impute_values = {}\n        \n    def fit(self, X, y=None):\n        num_cols = [col for col in X.columns if X[col].dtype != 'object' \n                   and col not in ['isFraud', 'TransactionID']]\n        \n        if self.num_strategy == 'median':\n            for col in num_cols:\n                self.num_impute_values[col] = X[col].median()\n        elif self.num_strategy == 'mean':\n            for col in num_cols:\n                self.num_impute_values[col] = X[col].mean()\n        elif self.num_strategy == 'constant':\n            for col in num_cols:\n                self.num_impute_values[col] = 0 \n                \n        cat_cols = [col for col in X.columns if X[col].dtype == 'object']\n        if self.cat_strategy == 'missing':\n            for col in cat_cols:\n                self.cat_impute_values[col] = 'missing'\n        elif self.cat_strategy == 'mode':\n            for col in cat_cols:\n                self.cat_impute_values[col] = X[col].mode()[0]\n        elif self.cat_strategy == 'constant':\n            for col in cat_cols:\n                self.cat_impute_values[col] = self.fill_value\n                \n        return self\n    \n    def transform(self, X, y=None):\n        for col, val in self.num_impute_values.items():\n            if col in X.columns:\n                X[col].fillna(val, inplace=True)\n                X[f'{col}_missing'] = X[col].isna().astype(int)\n                \n        for col, val in self.cat_impute_values.items():\n            if col in X.columns:\n                X[col].fillna(val, inplace=True)\n                \n        return X","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Feature Engineer Class (Creates new features and transforms existing ones\n","metadata":{}},{"cell_type":"code","source":"class FeatureEngineer(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        self.freq_encoders = {}\n        \n    def fit(self, X, y=None):\n        freq_cols = ['card1', 'card2', 'card3', 'card5', 'addr1', 'addr2']\n        for col in freq_cols:\n            if col in X.columns:\n                self.freq_encoders[col] = X[col].value_counts(dropna=False)\n        return self\n    \n    def transform(self, X, y=None):\n        if 'TransactionAmt' in X.columns:\n            for group_col in ['card1', 'card4']:\n                if group_col in X.columns:\n                    group_means = X.groupby([group_col])['TransactionAmt'].mean()\n                    X[f'TransactionAmt_to_mean_{group_col}'] = X['TransactionAmt'] / X[group_col].map(group_means)\n        \n        for col, counts in self.freq_encoders.items():\n            if col in X.columns:\n                X[f'{col}_freq'] = X[col].map(counts)\n        \n        if 'P_emaildomain' in X.columns and 'R_emaildomain' in X.columns:\n            X['P_emaildomain_match_R_emaildomain'] = (X['P_emaildomain'] == X['R_emaildomain']).astype(int)\n            \n        return X","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Handles unseen categories and missing values safely","metadata":{}},{"cell_type":"code","source":"class CategoricalEncoder(BaseEstimator, TransformerMixin):\n  \n    def __init__(self, handle_unseen='impute', impute_value='missing'):\n        self.handle_unseen = handle_unseen \n        self.impute_value = impute_value\n        self.encoders = {}\n        self.known_categories = {}\n        \n    def fit(self, X, y=None):\n        cat_cols = [col for col in X.columns if X[col].dtype == 'object']\n        for col in cat_cols:\n            unique_vals = X[col].dropna().unique()\n            self.known_categories[col] = set(unique_vals)\n            \n            le = LabelEncoder()\n            if self.handle_unseen == 'impute':\n                le.fit(np.append(unique_vals, self.impute_value))\n            else:\n                le.fit(unique_vals)\n            self.encoders[col] = le\n        return self\n    \n    def transform(self, X, y=None):\n        X = X.copy()\n        for col, le in self.encoders.items():\n            if col in X.columns:\n                X[col] = X[col].astype(str)\n                X[col] = X[col].replace('nan', self.impute_value)\n                \n                if self.handle_unseen == 'impute':\n                    unseen_mask = ~X[col].isin(self.known_categories[col])\n                    X.loc[unseen_mask, col] = self.impute_value\n                \n                try:\n                    X[col] = le.transform(X[col])\n                except ValueError:\n                    unseen = set(X[col].unique()) - set(le.classes_)\n                    X.loc[X[col].isin(unseen), col] = self.impute_value\n                    X[col] = le.transform(X[col])\n        return X","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Featire Selector Class (Selects features based on importance or other criteria)","metadata":{}},{"cell_type":"code","source":"class FeatureSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, strategy='default'):\n        self.strategy = strategy\n        self.selected_features = None\n        \n    def fit(self, X, y=None):\n        if self.strategy == 'default':\n            cols_to_drop = [\n                'TransactionID', \n                'V300', 'V309', 'V111', 'V124', 'V106', 'V125', 'V315', 'V134', 'V102', 'V123', \n                'V136', 'V305', 'V110', 'V129', 'V114', 'V116', 'V298', 'V126', 'V113', 'V105', \n                'V119', 'V104', 'V122', 'V320', 'V115', 'V317', 'V303', 'V112', 'V118', 'V108', \n                'V127', 'V132', 'V109', 'V103', 'V120', 'V107', 'V131', 'V135', 'V308', 'V117', \n                'V121', 'V133', 'V130', 'V318', 'V304', 'V128', 'V319', 'V307', 'V306', 'V302', \n                'V311', 'V301', 'V310'\n            ]\n            self.selected_features = [col for col in X.columns if col not in cols_to_drop]\n            \n        return self\n    \n    def transform(self, X, y=None):\n        if self.selected_features is not None:\n            X = X.copy()\n            for col in self.selected_features:\n                if col not in X.columns:\n                    X[col] = np.nan \n            return X[self.selected_features]\n        return X","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"## Start of training","metadata":{}},{"cell_type":"code","source":"RANDOM_STATE = 42\nN_FOLDS = 5\nTEST_SIZE = 0.2\n\n\ntrain_identity = pd.read_csv(\"/kaggle/input/ieee-fraud-detection/train_identity.csv\")\ntrain_transaction = pd.read_csv(\"/kaggle/input/ieee-fraud-detection/train_transaction.csv\")\n\n\ntrain_df = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')\n\nprint(f\"Training data shape: {train_df.shape}\")\nprint(f\"Fraud rate: {train_df['isFraud'].mean():.4f}\")\n\nX_train, X_val, y_train, y_val = train_test_split(\n    train_df.drop('isFraud', axis=1),\n    train_df['isFraud'],\n    test_size=TEST_SIZE,\n    random_state=RANDOM_STATE,\n    stratify=train_df['isFraud']\n)\n\n\nprint(\"Building preprocessing pipeline...\")\npreprocessing_pipeline = Pipeline([\n    ('memory_reducer', MemoryReducer()),\n    ('missing_handler', MissingValueHandler(\n        num_strategy='median',\n        cat_strategy='constant',\n        fill_value='missing'\n    )),\n    ('feature_engineer', FeatureEngineer()),\n    ('categorical_encoder', CategoricalEncoder(\n        handle_unseen='impute',\n        impute_value='missing'\n    )),\n    ('feature_selector', FeatureSelector(strategy='default'))\n])\n\nprint(\"Preprocessing training data...\")\nX_train_preprocessed = preprocessing_pipeline.fit_transform(X_train, y_train)\nX_val_preprocessed = preprocessing_pipeline.transform(X_val)\n\nprint(\"Training XGBoost model...\")\n\n\nkf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n\n\ncv_scores = []\nmodels = []\n\n\nfor fold, (train_idx, valid_idx) in enumerate(kf.split(X_train_preprocessed, y_train)):\n    print(f\"\\nFold {fold + 1}/{N_FOLDS}\")\n    \n    X_tr, X_v = X_train_preprocessed.iloc[train_idx], X_train_preprocessed.iloc[valid_idx]\n    y_tr, y_v = y_train.iloc[train_idx], y_train.iloc[valid_idx]\n    \n    model = XGBClassifier(\n        n_estimators=1000,\n        max_depth=6,\n        learning_rate=0.05,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        missing=-999,\n        random_state=RANDOM_STATE,\n        eval_metric='auc',\n        tree_method='hist'  \n    )\n    \n    model.fit(\n        X_tr, y_tr,\n        eval_set=[(X_v, y_v)],\n        early_stopping_rounds=100,\n        verbose=100\n    )\n    \n    val_preds = model.predict_proba(X_v)[:, 1]\n    score = roc_auc_score(y_v, val_preds)\n    cv_scores.append(score)\n    models.append(model)\n    print(f\"Fold {fold + 1} AUC: {score:.5f}\")\n\nprint(\"\\nCross-validation results:\")\nprint(f\"Mean AUC: {np.mean(cv_scores):.5f}\")\nprint(f\"Std AUC: {np.std(cv_scores):.5f}\")\n\nprint(\"\\nEvaluating on holdout validation set...\")\nval_preds = np.mean([model.predict_proba(X_val_preprocessed)[:, 1] for model in models], axis=0)\nval_score = roc_auc_score(y_val, val_preds)\nprint(f\"Validation AUC: {val_score:.5f}\")\n\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n## Feature Importance\n","metadata":{}},{"cell_type":"code","source":"\nprint(\"\\nFeature importance analysis...\")\nfeature_importance = pd.DataFrame({\n    'feature': X_train_preprocessed.columns,\n    'importance': np.mean([model.feature_importances_ for model in models], axis=0)\n}).sort_values('importance', ascending=False)\n\n\nprint(\"\\nTop 20 features:\")\nprint(feature_importance.head(20))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## MLFlow logging","metadata":{}},{"cell_type":"code","source":"mlflow.set_experiment(\"XGBoost_Training\")\n\n# 1. Cleaning\nwith mlflow.start_run(run_name=\"XGBoost_Cleaning\"):\n    mlflow.log_param(\"memory_reduction\", True)\n    mlflow.log_param(\"missing_value_strategy_num\", \"median\")\n    mlflow.log_param(\"missing_value_strategy_cat\", \"constant_missing\")\n    mlflow.log_text(\"Used MemoryReducer and MissingValueHandler for missing values.\", \"cleaning_notes.txt\")\n\nmlflow.end_run()\n\nwith mlflow.start_run(run_name=\"XGBoost_Feature_Engineering\"):\n    mlflow.log_param(\"feature_engineering_applied\", True)\n    mlflow.log_param(\"freq_encoding_columns\", ['card1', 'card2', 'card3', 'card5', 'addr1', 'addr2'])\n    mlflow.log_text(\"Created TransactionAmt_to_mean features and Email domain matching.\", \"feature_engineering_notes.txt\")\n\nmlflow.end_run()\n\nwith mlflow.start_run(run_name=\"XGBoost_Feature_Selection\"):\n    mlflow.log_param(\"feature_selection_strategy\", \"manual_drop_low_importance_V-features\")\n    mlflow.log_param(\"final_selected_features\", len(preprocessing_pipeline.named_steps['feature_selector'].selected_features))\n    mlflow.log_text(\"Dropped around 50 V-features manually based on prior EDA.\", \"feature_selection_notes.txt\")\n\nmlflow.end_run()\n\nwith mlflow.start_run(run_name=\"XGBoost_Final_Model\") as run:\n    mlflow.log_params({\n        \"model_type\": \"XGBoost\",\n        \"n_folds\": N_FOLDS,\n        \"random_state\": RANDOM_STATE,\n        \"n_estimators\": 1000,\n        \"max_depth\": 6,\n        \"learning_rate\": 0.05,\n        \"subsample\": 0.8,\n        \"colsample_bytree\": 0.8,\n    })\n    mlflow.log_metrics({\n        \"fold_1_auc\": cv_scores[0],\n        \"fold_2_auc\": cv_scores[1],\n        \"fold_3_auc\": cv_scores[2],\n        \"fold_4_auc\": cv_scores[3],\n        \"fold_5_auc\": cv_scores[4],\n        \"mean_auc\": np.mean(cv_scores),\n        \"std_auc\": np.std(cv_scores),\n        \"validation_auc\": val_score\n    })\n\n    best_model_idx = np.argmax(cv_scores)\n    best_model = models[best_model_idx]\n\n    final_pipeline = Pipeline([\n        ('preprocessing', preprocessing_pipeline),\n        ('model', best_model)\n    ])\n    \n    mlflow.xgboost.log_model(best_model, \"best_xgboost_model\")\n    mlflow.sklearn.log_model(final_pipeline, \"full_pipeline\")\n\n    top_features = feature_importance.head(20).to_dict()\n    mlflow.log_dict(top_features, \"feature_importance/top_20_features.json\")\n    \n    val_preds_sample = pd.DataFrame({\n        'actual': y_val,\n        'predicted': val_preds\n    }).sample(1000)\n    mlflow.log_table(val_preds_sample, \"validation_predictions_sample.json\")\n\n    mlflow.log_text(f\"\"\"\n    - Train shape: {X_train_preprocessed.shape}\n    - Fraud rate: {y_train.mean():.4f}\n    - Best fold AUC: {cv_scores[best_model_idx]:.4f}\n    - Validation AUC: {val_score:.4f}\n    \"\"\", \"training_summary.txt\")\n\n    print(f\"Successfully logged to MLflow! Run ID: {run.info.run_id}\")\n\nmlflow.end_run()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}